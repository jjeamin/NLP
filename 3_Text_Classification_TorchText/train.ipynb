{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import optim, nn\n",
    "from pathlib import Path\n",
    "from torchtext import data\n",
    "from torchtext.vocab import Vectors\n",
    "from konlpy.tag import Okt\n",
    "from tqdm import tqdm\n",
    "from torch.nn import functional as F\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion, device=\"cuda\"):\n",
    "    model.train()\n",
    "\n",
    "    total = len(train_loader)\n",
    "    train_correct = 0\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, total=total):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        data = batch.text.to(device)\n",
    "        labels = batch.label.to(device)\n",
    "\n",
    "        pred = model(data).squeeze(1)\n",
    "        pred = pred.detach().cpu().numpy()\n",
    "        predicted = np.round(pred > 0.5)\n",
    "\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        loss = criterion(pred.float(), labels.float())\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return train_correct, train_loss\n",
    "\n",
    "\n",
    "def valid(model, valid_loader, criterion, device=\"cuda\"):\n",
    "    model.eval()\n",
    "\n",
    "    total = len(valid_loader)\n",
    "    valid_correct = 0\n",
    "    valid_loss = 0\n",
    "\n",
    "    for batch in tqdm(valid_loader, total=total):\n",
    "        data = batch.text.to(device)\n",
    "        labels = batch.label.to(device)\n",
    "\n",
    "        pred = model(data).squeeze(1)\n",
    "        pred = pred.detach().cpu().numpy()\n",
    "        predicted = np.round(pred > 0.5)\n",
    "\n",
    "        valid_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        loss = criterion(pred.float(), labels.float())\n",
    "        valid_loss += loss.item()\n",
    "\n",
    "    return valid_correct, valid_loss\n",
    "\n",
    "\n",
    "def test(model, test_loader, device=\"cuda\"):\n",
    "    model.eval()\n",
    "\n",
    "    total = len(test_loader)\n",
    "\n",
    "    total_pred = []\n",
    "\n",
    "    for batch in tqdm(test_loader, total=total):\n",
    "        data = batch.text.to(device)\n",
    "\n",
    "        pred = model(data).squeeze(1)\n",
    "        pred = pred.detach().cpu().numpy()\n",
    "        predicted = np.round(pred > 0.5)[0]\n",
    "\n",
    "        total_pred.append(predicted)\n",
    "\n",
    "    return total_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"data\")\n",
    "w2v_file = \"./data/glove.txt\"\n",
    "\n",
    "train_df = pd.read_csv(DATA_PATH / \"news_train.csv\")\n",
    "test_df = pd.read_csv(DATA_PATH / \"news_test.csv\")\n",
    "submission = pd.read_csv(DATA_PATH / \"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Okt().morphs\n",
    "\n",
    "TEXT = data.Field(sequential=True,\n",
    "                  tokenize=tokenizer, # 토크나이저로는 Okt 사용.\n",
    "                  lower=True)\n",
    "\n",
    "LABEL = data.Field(sequential=False,\n",
    "                   use_vocab=False)\n",
    "\n",
    "train_datafields = [('id', None), ('date', None), ('title', None), ('text', TEXT), ('ord', None), ('label', LABEL)]\n",
    "test_datafields = [('id', None), ('date', None), ('title', None), ('text', TEXT), ('ord', None), ('label', None)]\n",
    "\n",
    "train_examples = [data.Example.fromlist(i, train_datafields) for i in train_df.values.tolist()]\n",
    "train_dataset = data.Dataset(train_examples, train_datafields)\n",
    "\n",
    "test_examples = [data.Example.fromlist(i, test_datafields) for i in test_df.values.tolist()]\n",
    "test_dataset = data.Dataset(test_examples, test_datafields)\n",
    "\n",
    "TEXT.build_vocab(train_dataset, vectors=Vectors(w2v_file),)\n",
    "word_embeddings = TEXT.vocab.vectors\n",
    "vocab = TEXT.vocab\n",
    "\n",
    "train_dataset, valid_dataset = train_dataset.split(split_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper params\n",
    "batch_size = 128\n",
    "lr = 0.01\n",
    "epochs = 10\n",
    "embedding_dim = 100\n",
    "vocab_size = len(vocab)\n",
    "device = 'cuda'\n",
    "save_path = 'model.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = data.BucketIterator((train_dataset),\n",
    "                                batch_size=batch_size,\n",
    "                                sort_key=lambda x: len(x.text),\n",
    "                                repeat=False,\n",
    "                                shuffle=True)\n",
    "        \n",
    "valid_iter = data.BucketIterator((valid_dataset),\n",
    "                                batch_size=batch_size,\n",
    "                                sort_key=lambda x: len(x.text),\n",
    "                                repeat=False,\n",
    "                                shuffle=False)\n",
    "\n",
    "test_iter = data.BucketIterator((test_dataset),\n",
    "                                batch_size=1,\n",
    "                                sort_key=lambda x: len(x.text),\n",
    "                                repeat=False,\n",
    "                                shuffle=False)\n",
    "\n",
    "print(\"Loaded {} training examples\".format(len(train_dataset)))\n",
    "print(\"Loaded {} validation examples\".format(len(valid_dataset)))\n",
    "print(\"Loaded {} testing examples\".format(len(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RCNN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 embedding_dim, \n",
    "                 vocab_size, \n",
    "                 num_layers = 1,\n",
    "                 hidden_size=64, \n",
    "                 dropout=0.8,\n",
    "                 word_embeddings=None):\n",
    "\n",
    "        super(RCNN, self).__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        if word_embeddings is not None:\n",
    "            self.embeddings.weight = nn.Parameter(word_embeddings, requires_grad=False)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = embedding_dim,\n",
    "                            hidden_size = hidden_size,\n",
    "                            num_layers = num_layers,\n",
    "                            dropout = dropout,\n",
    "                            bidirectional = True)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.W = nn.Linear(embedding_dim + 2*hidden_size, 128)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.fc = nn.Linear(128, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape = (seq_len, batch_size)\n",
    "        embedded_sent = self.embeddings(x)\n",
    "        # embedded_sent.shape = (seq_len, batch_size, embed_size)\n",
    "\n",
    "        lstm_out, (h_n,c_n) = self.lstm(embedded_sent)\n",
    "        # lstm_out.shape = (seq_len, batch_size, 2 * hidden_size)\n",
    "        \n",
    "        input_features = torch.cat([lstm_out,embedded_sent], 2).permute(1,0,2)\n",
    "        # final_features.shape = (batch_size, seq_len, embed_size + 2*hidden_size)\n",
    "        \n",
    "        linear_output = self.tanh(\n",
    "            self.W(input_features)\n",
    "        )\n",
    "        # linear_output.shape = (batch_size, seq_len, hidden_size_linear)\n",
    "        \n",
    "        linear_output = linear_output.permute(0,2,1) # Reshaping fot max_pool\n",
    "        \n",
    "        max_out_features = F.max_pool1d(linear_output, linear_output.shape[2]).squeeze(2)\n",
    "        # max_out_features.shape = (batch_size, hidden_size_linear)\n",
    "        \n",
    "        max_out_features = self.dropout(max_out_features)\n",
    "        final_out = self.fc(max_out_features)\n",
    "        \n",
    "        return self.sigmoid(final_out)\n",
    "        # return final_out\n",
    "\n",
    "model = RCNN(embedding_dim=embedding_dim,\n",
    "             vocab_size=vocab_size,\n",
    "             word_embeddings=word_embeddings).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, [30, 60, 90], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0\n",
    "train_total = len(train_iter)\n",
    "valid_total = len(valid_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(0, epochs):\n",
    "    train_correct, train_loss = train(model, train_iter, optimizer, criterion, device=device)\n",
    "    train_acc = train_correct / (train_total * batch_size)\n",
    "    train_loss = train_loss / (train_total * batch_size)\n",
    "\n",
    "    valid_correct, valid_loss = valid(model, valid_iter, criterion, device=device)\n",
    "    valid_acc = valid_correct / (valid_total * batch_size)\n",
    "    valid_loss = valid_loss / (valid_total * batch_size)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"[EPOCH : {epochs} / {e}] || [TRAIN ACC : {train_acc}] || [TRAIN LOSS : {train_loss}]\"\n",
    "            f\"|| [VALID ACC : {valid_acc}] || [VALID LOSS : {valid_loss}]\")\n",
    "\n",
    "    if best_acc < valid_acc:\n",
    "        torch.save({'epoch': e,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict()},\n",
    "                    save_path)\n",
    "        best_acc = valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 142565/142565 [05:42<00:00, 416.54it/s]\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(save_path)['model_state_dict'])\n",
    "pred = test(model, test_iter, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['info'] = pred\n",
    "submission.to_csv('starter.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
